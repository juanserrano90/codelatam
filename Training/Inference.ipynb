{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuZnllykUjGsfkSZufSjhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanserrano90/codelatam/blob/main/Training/Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0MRQ8U6_zaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6aac928-4900-464d-8b06-0742f9b9abd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'codelatam'...\n",
            "remote: Enumerating objects: 75776, done.\u001b[K\n",
            "remote: Counting objects: 100% (3959/3959), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3948/3948), done.\u001b[K\n",
            "remote: Total 75776 (delta 25), reused 3934 (delta 11), pack-reused 71817 (from 2)\u001b[K\n",
            "Receiving objects: 100% (75776/75776), 696.42 MiB | 19.18 MiB/s, done.\n",
            "Resolving deltas: 100% (1285/1285), done.\n",
            "Updating files: 100% (90963/90963), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/juanserrano90/codelatam.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m-DXJXVDsOk",
        "outputId": "eeca2dae-04ed-46e1-98dd-cfac6bc567da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from transformers import AutoImageProcessor, Swinv2Model\n",
        "from transformers import DINOv3ViTModel\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "zLyzKQKmDsR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c055618a-7b85-475c-cc86-5bb55db692b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global definitions ---------------\n",
        "data_dir = \"/content/codelatam/Data\"\n",
        "working_dir = \"/content/drive/MyDrive/Doctorado/Codelatam/Files_codelatam\"\n",
        "num_classes = 3\n",
        "inv_dict_mapping_classes = {0:'Ia-norm', 1:'Ia-pec', 2:'Others'}\n",
        "dataset_folder = 'Dataset_augmented_images'\n",
        "\n",
        "def subtype_to_class_mapping(a):\n",
        "  subtype_to_class = {0:0, 1:1, 2:1, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:2, 10:2, 11:2, 12:2, 13:2, 14:2, 15:2, 16:2}\n",
        "  return subtype_to_class[a]\n",
        "\n",
        "def id_to_subtype_mapping(a):\n",
        "  id_to_subtype = {0: 'Ia-norm', 1: 'Ia-91T', 3: 'Ia-csm', 2: 'Ia-91bg', 6: 'Ib-norm', 4: 'Iax', 5: 'Ia-pec', 10: 'Ic-norm',\n",
        "                   13: 'IIP', 14: 'IIL', 8: 'IIb', 16: 'II-pec', 11: 'Ic-broad', 12: 'Ic-pec', 15: 'IIn', 7: 'Ibn', 9: 'Ib-pec'}\n",
        "  return id_to_subtype(a)\n",
        "\n",
        "dataset_versions = [\n",
        "    'augmented_images_v2.0',\n",
        "    'augmented_images_v2.0_20x20',\n",
        "    'augmented_images_v2.0_20x20_n',\n",
        "    'augmented_images_v2.0_224x112',\n",
        "    'augmented_images_v2.0_224x112_n',\n",
        "    'augmented_images_v2.0_224x224',\n",
        "    'augmented_images_v2.0_224x224_n',\n",
        "    'augmented_images_v2.0_224x56',\n",
        "    'augmented_images_v2.0_224x56_n',\n",
        "    'augmented_images_v2.0_50x50',\n",
        "    'augmented_images_v2.0_50x50_n',\n",
        "    ]"
      ],
      "metadata": {
        "id": "hi9k0FtdIC9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split(n):\n",
        "  with open(f\"{data_dir}/Splits/tvt_split{n}.pkl\", 'rb') as f:\n",
        "    splits = pickle.load(f)\n",
        "  return splits\n",
        "\n",
        "def extract_data_from_splits(splits, root_dir):\n",
        "    data = {'train': [], 'val': [], 'test': []}\n",
        "\n",
        "    for subfolder, split_dict in splits.items():\n",
        "        for split, image_list in split_dict.items():\n",
        "            for image_name in image_list:\n",
        "              if len(root_dir.split('/')[-1].split('_'))>3:   # dfdw datasets have different naming\n",
        "                if 'COPY' in image_name:\n",
        "                  image_name = image_name[:-8] + \"m_\" + \"COPY\" + \".png\"\n",
        "                else:\n",
        "                  image_name = image_name[:-4] + \"_m\" + \".png\"\n",
        "              image_path = os.path.join(root_dir, subfolder, image_name)\n",
        "              label = subtype_to_class_mapping(int(image_name.split('_')[1]))\n",
        "              image_id = image_name[:-4]\n",
        "              data[split].append((image_path, image_id, label))\n",
        "\n",
        "    return data['train'], data['val'], data['test']\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, processor):\n",
        "        \"\"\"\n",
        "        data: list of tuples (image_path, id, label)\n",
        "        \"\"\"\n",
        "        self.image_paths = [d[0] for d in data]\n",
        "        self.ids = [d[1] for d in data]\n",
        "        self.labels = [d[2] for d in data]\n",
        "        self.processor = processor\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = read_image(self.image_paths[idx], mode=ImageReadMode.RGB)\n",
        "        processed = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = processed['pixel_values'].squeeze(0)\n",
        "        label = torch.tensor(self.labels[idx]).long()\n",
        "        return {\n",
        "            'id': self.ids[idx],\n",
        "            'pixel_values': pixel_values,\n",
        "            'y_true': label\n",
        "        }\n",
        "\n",
        "def load_datasets(splits, dataset_name, processor):\n",
        "  train_data, val_data, test_data = extract_data_from_splits(splits, os.path.join(data_dir, dataset_folder, dataset_name))\n",
        "\n",
        "  train_dataset = CustomDataset(train_data, processor)\n",
        "  val_dataset = CustomDataset(val_data, processor)\n",
        "  test_dataset = CustomDataset(test_data, processor)\n",
        "\n",
        "  print(f'Loading {dataset_name}...')\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def show_example_image(dataset, n):\n",
        "  image_data = dataset[n]\n",
        "  # Denormalize the image using processor's mean and std\n",
        "  # pixel_values are (C, H, W)\n",
        "  mean = torch.tensor(processor.image_mean).view(3, 1, 1)\n",
        "  std = torch.tensor(processor.image_std).view(3, 1, 1)\n",
        "  denormalized_image = (image_data['pixel_values'] * std) + mean\n",
        "  # Convert to numpy array, scale to 0-255, and change to uint8\n",
        "  image_np = (denormalized_image.permute(1, 2, 0).numpy() * 255).astype('uint8')\n",
        "  img = Image.fromarray(image_np)\n",
        "  display(img)\n",
        "\n",
        "def show_model_architecture(model):\n",
        "  !pip install torchinfo\n",
        "  from torchinfo import summary\n",
        "  summary(model)\n",
        "\n",
        "def load_model_and_classifier(pt_model_name, dropout, head_n):\n",
        "\n",
        "  if 'google' in pt_model_name:\n",
        "    model = ViTModel.from_pretrained(pt_model_name).to(device)\n",
        "  elif 'microsoft' in pt_model_name:\n",
        "    model = Swinv2Model.from_pretrained(pt_model_name).to(device)\n",
        "  elif 'facebook' in pt_model_name:\n",
        "    model = DINOv3ViTModel.from_pretrained(pt_model_name).to(device)\n",
        "\n",
        "  if head_n == 3:\n",
        "    classifier = nn.Sequential(\n",
        "      nn.Linear(model.config.hidden_size, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(512, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(256, num_classes)\n",
        "      ).to(device)\n",
        "\n",
        "  elif head_n == 2:\n",
        "    classifier = nn.Sequential(\n",
        "      nn.Linear(model.config.hidden_size, 256),\n",
        "      nn.GELU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(256, num_classes)).to(device)\n",
        "\n",
        "  elif head_n == 1:\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Linear(model.config.hidden_size, num_classes)).to(device)\n",
        "\n",
        "  return model, classifier\n",
        "\n",
        "def train_step(batch_data, model, processor, classifier, optimizer, device):\n",
        "    model.train()\n",
        "    classifier.train()\n",
        "    y_true = batch_data['y_true'].to(device)\n",
        "\n",
        "    # inputs = processor(images=batch_data['pixel_values'], return_tensors=\"pt\", do_convert_rgb=False).to(device)\n",
        "    # outputs = model(**inputs)\n",
        "    inputs = batch_data['pixel_values'].to(device)\n",
        "    outputs = model(inputs)\n",
        "    # pooled_output = outputs.last_hidden_state[:, 0, :]             # alternatively\n",
        "    pooled_output = outputs.pooler_output\n",
        "\n",
        "    logits = classifier(pooled_output)\n",
        "    loss = F.cross_entropy(logits, y_true)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    y_pred = torch.argmax(logits, dim=-1)\n",
        "    return loss.item(), y_pred, y_true\n",
        "\n",
        "\n",
        "def validate_step(batch_data, model, processor, classifier, device):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        y_true = batch_data['y_true'].to(device)\n",
        "\n",
        "        inputs = batch_data['pixel_values'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        logits = classifier(pooled_output)\n",
        "        loss = F.cross_entropy(logits, y_true)\n",
        "\n",
        "        y_pred = torch.argmax(logits, dim=-1)\n",
        "        return loss.item(), y_pred, y_true\n",
        "\n",
        "\n",
        "def predict_step(batch_data, model, processor, classifier, device):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        y_true = batch_data['y_true'].to(device)\n",
        "\n",
        "        inputs = batch_data['pixel_values'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        logits = classifier(pooled_output)\n",
        "        y_pred_prob = F.softmax(logits, dim=1)\n",
        "        y_pred = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        return {\n",
        "            'id': batch_data['id'],\n",
        "            'y_pred': y_pred.cpu(),\n",
        "            'y_pred_prob': y_pred_prob.cpu(),\n",
        "            'y_true': y_true.cpu()\n",
        "        }\n",
        "\n",
        "def evaluate(\n",
        "    model,\n",
        "    classifier,\n",
        "    processor,\n",
        "    dataloader,\n",
        "    inv_class_map=None,\n",
        "    save_dir=None,\n",
        "    ):\n",
        "\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    y_pred_prob = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "          output = predict_step(batch, model, processor, classifier, device)\n",
        "          y_true.append(output['y_true'])\n",
        "          y_pred.append(output['y_pred'])\n",
        "          y_pred_prob.append(output['y_pred_prob'])\n",
        "\n",
        "    y_true = torch.cat(y_true).numpy()\n",
        "    y_pred = torch.cat(y_pred).numpy()\n",
        "    y_pred_prob = torch.cat(y_pred_prob).numpy()\n",
        "\n",
        "    if inv_class_map is not None:\n",
        "        y_true = [inv_class_map[i] for i in y_true]\n",
        "        y_pred = [inv_class_map[i] for i in y_pred]\n",
        "\n",
        "    # --- Classification report ---\n",
        "    report_str = classification_report(y_true, y_pred, digits=4)\n",
        "    print(\"\\n=== Classification Report ===\")\n",
        "    print(report_str)\n",
        "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    # --- Save to disk ---\n",
        "    if save_dir is not None:\n",
        "      file_name = f\"clfreport_bs_{hp['batch_size']}_lr_{hp['lr']}_wd_{hp['wd']}_dp_{hp['dropout']}_pt_{hp['patience']}_f_{hp['freeze']}_h{hp['head_n']}.txt\"\n",
        "      txt_path = os.path.join(f\"{working_dir}/Runs/{save_dir}\", file_name)\n",
        "      with open(txt_path, \"w\") as f:\n",
        "        f.write(report_str)\n",
        "\n",
        "    return y_true, y_pred, y_pred_prob, macro_f1\n",
        "\n",
        "def load_processor(pt_model_name):\n",
        "  if 'google' in pt_model_name:\n",
        "    processor = ViTImageProcessor.from_pretrained(pt_model_name)\n",
        "  elif 'microsoft' in pt_model_name:\n",
        "    processor = AutoImageProcessor.from_pretrained(pt_model_name, use_fast=True)\n",
        "  elif 'facebook' in pt_model_name:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "    processor = AutoImageProcessor.from_pretrained(pt_model_name, use_fast=True)\n",
        "\n",
        "  return processor\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    inv_dict_mapping_classes,\n",
        "    normalize=True,\n",
        "    figsize=(10, 7),\n",
        "    save_dir=None,\n",
        "    dpi=100\n",
        "):\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    class_names = inv_dict_mapping_classes.values()\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype(float)\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        cm = np.divide(cm, row_sums, where=row_sums != 0)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\".2f\" if normalize else \"d\",\n",
        "        cmap=\"Blues\",\n",
        "        cbar=False,\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    title = \"Confusion Matrix (Normalized)\" if normalize else \"Confusion Matrix\"\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_dir is not None:\n",
        "      file_name = f\"cfmatrix_{model_name}.png\"\n",
        "      save_path = os.path.join(f\"{working_dir}/Runs/{save_dir}\", file_name)\n",
        "      plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1z3MWub_IK7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load best_model_**.pth, set to eval_mode\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pt_model_names = {'ViT32': 'google/vit-base-patch32-224-in21k',\n",
        "                  'ViT16': 'google/vit-base-patch16-224-in21k',\n",
        "                  'Swinv2': 'microsoft/swinv2-tiny-patch4-window16-256',\n",
        "                  'DINOv3': 'facebook/dinov3-vits16-pretrain-lvd1689m'}\n",
        "final_models = {\n",
        "    \"ViT16\": {\"bs\": 64, \"dropout\": 0.1, \"wd\": 1e-4, \"file_name\": \"model_bs_64_lr_1e-05_wd_0.0001_dp_0.1_pt_10_f_False_h3.pth\"},\n",
        "    \"ViT32\": {\"bs\": 32, \"dropout\": 0.5, \"wd\": 1e-4, \"file_name\": \"model_bs_32_lr_1e-05_wd_0.0001_dp_0.5_pt_10_f_False_h3.pth\"},\n",
        "    \"Swinv2\": {\"bs\": 32, \"dropout\": 0.3, \"wd\": 1e-4, \"file_name\": \"model_bs_32_lr_1e-05_wd_0.0001_dp_0.3_pt_10_f_False_h3.pth\"},\n",
        "    \"DINOv3\": {\"bs\": 32, \"dropout\": 0.5, \"wd\": 1e-5, \"file_name\": \"model_bs_32_lr_1e-05_wd_1e-05_dp_0.5_pt_10_f_False_h3.pth\"},\n",
        "}\n",
        "\n",
        "def run_inference(model_name, n_split, save_dir):\n",
        "\n",
        "  dataset_name = dataset_versions[0]\n",
        "  pt_model_name = pt_model_names[model_name]\n",
        "  splits = load_split(n_split)\n",
        "  processor = load_processor(pt_model_name)\n",
        "\n",
        "  train_dataset, val_dataset, test_dataset = load_datasets(\n",
        "      splits,\n",
        "      dataset_name,\n",
        "      processor,\n",
        "      )\n",
        "\n",
        "  test_dataloader = DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=final_models[model_name]['bs'],\n",
        "      )\n",
        "\n",
        "  model, classifier = load_model_and_classifier(\n",
        "      pt_model_name,\n",
        "      final_models[model_name]['dropout'],\n",
        "      head_n=3\n",
        "      )\n",
        "\n",
        "  best_state = torch.load(f\"{working_dir}/Runs/Phase4/{model_name}/v2.0/split4/{final_models[model_name][\"file_name\"]}\")\n",
        "  model.load_state_dict(best_state['model'])\n",
        "  classifier.load_state_dict(best_state['classifier'])\n",
        "\n",
        "  model.eval()\n",
        "  classifier.eval()\n",
        "\n",
        "  outputs = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for batch_data in test_dataloader:\n",
        "        outputs.append(predict_step(batch_data, model, processor, classifier, device))\n",
        "\n",
        "  keys = outputs[0].keys()\n",
        "  outputs_dict = {key: [] for key in keys}\n",
        "\n",
        "  for i in range(len(outputs)):\n",
        "      for key in keys:\n",
        "          outputs_dict[key].append(outputs[i][key])\n",
        "\n",
        "  outputs_dict = {key: np.concatenate(values) for key, values in outputs_dict.items()}\n",
        "\n",
        "  outputs_dict['y_true_c'] = [inv_dict_mapping_classes[val] for val in outputs_dict['y_true']]\n",
        "  outputs_dict['y_pred_c'] = [inv_dict_mapping_classes[val] for val in outputs_dict['y_pred']]\n",
        "\n",
        "    # --- Classification report ---\n",
        "  report_str = classification_report(outputs_dict['y_true_c'], outputs_dict['y_pred_c'], digits=4)\n",
        "  print(\"\\n=== Classification Report ===\")\n",
        "  print(report_str)\n",
        "  macro_f1 = f1_score(outputs_dict['y_true_c'], outputs_dict['y_pred_c'], average=\"macro\")\n",
        "\n",
        "  # --- Save to disk ---\n",
        "  if save_dir is not None:\n",
        "    save_path = os.path.join(working_dir, \"Runs\", save_dir)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    file_name = f\"clfreport_{model_name}.txt\"\n",
        "    txt_path = os.path.join(f\"{working_dir}/Runs/{save_dir}\", file_name)\n",
        "    with open(txt_path, \"w\") as f:\n",
        "      f.write(report_str)\n",
        "\n",
        "    with open(f\"{working_dir}/Runs/{save_dir}/output_dict_\"+model_name+\".pkl\", \"wb\") as f:\n",
        "      pickle.dump(outputs_dict, f)\n",
        "\n",
        "  plot_confusion_matrix(\n",
        "    outputs_dict['y_true'], outputs_dict['y_pred'],\n",
        "    inv_dict_mapping_classes,\n",
        "    figsize=(5,5),\n",
        "    save_dir=save_dir,\n",
        "    )\n",
        "\n",
        "  return outputs_dict"
      ],
      "metadata": {
        "id": "KbYpslBADsZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"DINOv3\"\n",
        "n_split = 4\n",
        "save_dir = \"Test\"\n",
        "\n",
        "# outputs_dict = run_inference(model_name, n_split, save_dir)"
      ],
      "metadata": {
        "id": "za6vT_7LA3D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble\n",
        "outputs_dicts = {}\n",
        "\n",
        "for m in final_models.keys():\n",
        "  with open(f\"{working_dir}/Runs/Test/output_dict_\"+m+\".pkl\", 'rb') as f:\n",
        "    outputs_dicts[m] = pickle.load(f)\n",
        "\n",
        "with open(f\"{working_dir}/Runs/Test/output_dict_ensemble_soft.pkl\", 'rb') as f:\n",
        "  outputs_dicts[\"Ensemble\"] = pickle.load(f)\n",
        "\n",
        "y_true = np.array(outputs_dicts[\"ViT16\"][\"y_true\"])\n",
        "\n",
        "preds = {\n",
        "    \"ViT16\": np.array(outputs_dicts[\"ViT16\"][\"y_pred\"]),\n",
        "    \"Swinv2\": np.array(outputs_dicts[\"Swinv2\"][\"y_pred\"]),\n",
        "    \"DINOv3\": np.array(outputs_dicts[\"DINOv3\"][\"y_pred\"]),\n",
        "    \"ViT32\": np.array(outputs_dicts[\"ViT32\"][\"y_pred\"]),\n",
        "    \"Ensemble\": np.array(outputs_dicts[\"ViT16\"][\"y_pred\"]),\n",
        "}\n",
        "\n",
        "probs = {\n",
        "    \"ViT16\": np.array(outputs_dicts[\"ViT16\"][\"y_pred_prob\"]),\n",
        "    \"Swinv2\": np.array(outputs_dicts[\"Swinv2\"][\"y_pred_prob\"]),\n",
        "    \"DINOv3\": np.array(outputs_dicts[\"DINOv3\"][\"y_pred_prob\"]),\n",
        "    \"ViT32\": np.array(outputs_dicts[\"ViT32\"][\"y_pred_prob\"]),\n",
        "    \"Ensemble\": np.array(outputs_dicts[\"ViT16\"][\"y_pred_prob\"]),\n",
        "}"
      ],
      "metadata": {
        "id": "IT514Qz7Cc9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "# Hard and soft ensembles\n",
        "# pred_matrix = np.stack(list(preds.values()), axis=0).astype(np.int64)\n",
        "# y_pred_hard, _ = mode(pred_matrix, axis=0, keepdims=False)\n",
        "\n",
        "# avg_probs = np.mean(np.stack(list(probs.values()), axis=0), axis=0)\n",
        "# y_pred_soft = np.argmax(avg_probs, axis=1)\n",
        "\n",
        "# y_pred_soft_c = [inv_dict_mapping_classes[val] for val in y_pred_soft]\n",
        "# y_pred_soft_c\n",
        "\n",
        "# print(\"=== Hard voting (3 models) ===\")\n",
        "# print(classification_report(y_true, y_pred_hard, digits=4))\n",
        "\n",
        "# print(\"=== Soft voting (3 models) ===\")\n",
        "# print(classification_report(y_true, y_pred_soft, digits=4))\n",
        "\n",
        "# soft_output_dict = {\"y_true\": y_true,\n",
        "#                     \"y_pred\": y_pred_soft,\n",
        "#                     \"y_pred_prob\": avg_probs,\n",
        "#                     \"y_pred_c\": y_pred_soft_c}\n",
        "\n",
        "# normalize=True\n",
        "# figsize=(6,6)\n",
        "#   # --- Classification report ---\n",
        "# report_str = classification_report(y_true, y_pred_soft, digits=4)\n",
        "# # --- Save to disk ---\n",
        "# save_path = os.path.join(working_dir, \"Runs\", \"Test\")\n",
        "# file_name = f\"clfreport_ensemble_soft.txt\"\n",
        "# txt_path = os.path.join(f\"{working_dir}/Runs/Test\", file_name)\n",
        "# with open(txt_path, \"w\") as f:\n",
        "#   f.write(report_str)\n",
        "\n",
        "# with open(f\"{working_dir}/Runs/Test/output_dict_ensemble_soft.pkl\", \"wb\") as f:\n",
        "#   pickle.dump(soft_output_dict, f)\n",
        "\n",
        "# cm = confusion_matrix(y_true, y_pred_soft)\n",
        "# class_names = inv_dict_mapping_classes.values()\n",
        "\n",
        "# if normalize:\n",
        "#     cm = cm.astype(float)\n",
        "#     row_sums = cm.sum(axis=1, keepdims=True)\n",
        "#     cm = np.divide(cm, row_sums, where=row_sums != 0)\n",
        "\n",
        "# plt.figure(figsize=figsize)\n",
        "\n",
        "# sns.heatmap(\n",
        "#     cm,\n",
        "#     annot=True,\n",
        "#     fmt=\".2f\" if normalize else \"d\",\n",
        "#     cmap=\"Blues\",\n",
        "#     cbar=False,\n",
        "#     xticklabels=class_names,\n",
        "#     yticklabels=class_names\n",
        "# )\n",
        "\n",
        "# plt.xlabel(\"Predicted Labels\")\n",
        "# plt.ylabel(\"True Labels\")\n",
        "# title = \"Confusion Matrix (Normalized)\" if normalize else \"Confusion Matrix\"\n",
        "# plt.title(title)\n",
        "\n",
        "# plt.tight_layout()\n",
        "\n",
        "\n",
        "# file_name = f\"cfmatrix_ensemble_soft.png\"\n",
        "# save_path = os.path.join(f\"{working_dir}/Runs/Test\", file_name)\n",
        "# plt.savefig(save_path, dpi=100, bbox_inches=\"tight\")\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ifeTHs0oKBD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(f\"--------- \\t ROC AUC (ovo weighted) \\t ROC AUC (ovr weighted)\")\n",
        "for m in outputs_dicts.keys():\n",
        "  r = roc_auc_score(outputs_dicts[m]['y_true'], outputs_dicts[m]['y_pred_prob'], multi_class='ovo', average=\"weighted\")\n",
        "  r2 = roc_auc_score(outputs_dicts[m]['y_true'], outputs_dicts[m]['y_pred_prob'], multi_class='ovr', average=\"weighted\")\n",
        "  print(f\"{m:<10} \\t {r:.2f}                       \\t {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o9lAGspsmvS",
        "outputId": "bfc7af73-fe5e-468c-e039-118463ff0c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- \t ROC AUC (ovo weighted) \t ROC AUC (ovr weighted)\n",
            "ViT16      \t 0.95                       \t 0.95\n",
            "ViT32      \t 0.94                       \t 0.95\n",
            "Swinv2     \t 0.97                       \t 0.97\n",
            "DINOv3     \t 0.95                       \t 0.95\n",
            "Ensemble   \t 0.97                       \t 0.98\n"
          ]
        }
      ]
    }
  ]
}