{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzALYZv9r987KNO6VlUaqX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/juanserrano90/codelatam.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OdEqA0hXeIL",
        "outputId": "ca1bed22-7801-4b42-c774-8789e9bcdcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'codelatam'...\n",
            "remote: Enumerating objects: 75745, done.\u001b[K\n",
            "remote: Counting objects: 100% (3928/3928), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3923/3923), done.\u001b[K\n",
            "remote: Total 75745 (delta 5), reused 3923 (delta 5), pack-reused 71817 (from 2)\u001b[K\n",
            "Receiving objects: 100% (75745/75745), 693.30 MiB | 24.19 MiB/s, done.\n",
            "Resolving deltas: 100% (1265/1265), done.\n",
            "Updating files: 100% (90957/90957), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPjxvUwEXFXS",
        "outputId": "2f74df24-9e24-4827-d082-afaa791aabb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global definitions ---------------\n",
        "data_dir = \"/content/codelatam/Data\"\n",
        "working_dir = \"/content/drive/MyDrive/Doctorado/Codelatam/Files_codelatam\"\n",
        "num_classes = 3\n",
        "inv_dict_mapping_classes = {0:'Ia-norm', 1:'Ia-pec', 2:'Others'}\n",
        "dataset_folder = 'Dataset_augmented_images'\n",
        "\n",
        "def subtype_to_class_mapping(a):\n",
        "  subtype_to_class = {0:0, 1:1, 2:1, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:2, 10:2, 11:2, 12:2, 13:2, 14:2, 15:2, 16:2}\n",
        "  return subtype_to_class[a]\n",
        "\n",
        "def id_to_subtype_mapping(a):\n",
        "  id_to_subtype = {0: 'Ia-norm', 1: 'Ia-91T', 3: 'Ia-csm', 2: 'Ia-91bg', 6: 'Ib-norm', 4: 'Iax', 5: 'Ia-pec', 10: 'Ic-norm',\n",
        "                   13: 'IIP', 14: 'IIL', 8: 'IIb', 16: 'II-pec', 11: 'Ic-broad', 12: 'Ic-pec', 15: 'IIn', 7: 'Ibn', 9: 'Ib-pec'}\n",
        "  return id_to_subtype[a]\n",
        "\n",
        "# Target ratios for Train, Val, Test\n",
        "SPLIT_RATIOS = {'train': 0.8, 'val': 0.1, 'test': 0.1}"
      ],
      "metadata": {
        "id": "p6yNIE3yXLEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split(n):\n",
        "  with open(f\"{data_dir}/Splits/saved_train_val_test_split_{n}.pkl\", 'rb') as f:\n",
        "    splits = pickle.load(f)\n",
        "  return splits\n",
        "def load_split_drive(n):\n",
        "  with open(f\"/content/drive/MyDrive/Doctorado/Codelatam/Files_codelatam/Splits/saved_train_val_test_split_{n}.pkl\", 'rb') as f:\n",
        "    splits = pickle.load(f)\n",
        "  return splits\n",
        "def verify_split_stats(split_results):\n",
        "    print(\n",
        "        f\"{'Class':<6} | {'Set':<6} | {'Images':<6} | {'%':<5} | \"\n",
        "        f\"{'Unique SNs':<10} | {'Subtypes':<8} | {'COPY imgs'}\"\n",
        "    )\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for cls in split_results:\n",
        "        total_imgs = sum(len(split_results[cls][s]) for s in split_results[cls])\n",
        "\n",
        "        for s in ['train', 'val', 'test']:\n",
        "            imgs = split_results[cls][s]\n",
        "            n_imgs = len(imgs)\n",
        "            pct = (n_imgs / total_imgs * 100) if total_imgs > 0 else 0\n",
        "\n",
        "            sns = [get_sn_info(i)[0] for i in imgs]\n",
        "            subtypes = [get_sn_info(i)[1] for i in imgs]\n",
        "\n",
        "            unique_sns = set(sns)\n",
        "            unique_subtypes = set(st for st in subtypes if st is not None)\n",
        "\n",
        "            n_copy = sum(\"COPY\" in img for img in imgs)\n",
        "\n",
        "            print(\n",
        "                f\"{cls:<6} | {s:<6} | {n_imgs:<6} | {pct:4.1f}% | \"\n",
        "                f\"{len(unique_sns):<10} | {len(unique_subtypes):<8} | {n_copy}\"\n",
        "            )\n",
        "        print(\"-\" * 90)\n",
        "\n",
        "def get_sn_info(filename):\n",
        "    try:\n",
        "        parts = filename.split('_')\n",
        "        sn_name = parts[0]\n",
        "        subtype = int(parts[1])\n",
        "        return sn_name, subtype\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "# This was used before.. Not used now\n",
        "def intelligent_split(all_images_dict, ratios):\n",
        "    \"\"\"\n",
        "    Splits data maintaining:\n",
        "    1. No SN leakage (Unique SNs in one set only)\n",
        "    2. Balanced Subtypes (Subtypes distributed across sets)\n",
        "    3. Balanced Image Counts (Target ratios respected)\n",
        "    4. Heavy SNs prioritize Train (via greedy capacity filling)\n",
        "    \"\"\"\n",
        "    final_split = {}\n",
        "\n",
        "    # 1. ORGANIZE DATA\n",
        "    # Structure: structured_data[class][subtype] = [ {'name': sn1, 'imgs': [...]}, ... ]\n",
        "    structured_data = {}\n",
        "\n",
        "    for cls, img_list in all_images_dict.items():\n",
        "        structured_data[cls] = {}\n",
        "\n",
        "        # Group images by SN first\n",
        "        sn_groups = {}\n",
        "        for img in img_list:\n",
        "            sn_name, subtype = get_sn_info(img)\n",
        "            if sn_name is None: continue\n",
        "\n",
        "            if sn_name not in sn_groups:\n",
        "                sn_groups[sn_name] = {'subtype': subtype, 'images': []}\n",
        "            sn_groups[sn_name]['images'].append(img)\n",
        "\n",
        "        # Regroup by Subtype\n",
        "        for sn, data in sn_groups.items():\n",
        "            st = data['subtype']\n",
        "            if st not in structured_data[cls]:\n",
        "                structured_data[cls][st] = []\n",
        "\n",
        "            # Store object with image count for sorting later\n",
        "            structured_data[cls][st].append({\n",
        "                'sn_name': sn,\n",
        "                'images': data['images'],\n",
        "                'count': len(data['images'])\n",
        "            })\n",
        "\n",
        "    # 2. PERFORM SPLIT\n",
        "    for cls in structured_data:\n",
        "        final_split[cls] = {'train': [], 'val': [], 'test': []}\n",
        "\n",
        "        # Track current image counts for this class to guide the greedy algorithm\n",
        "        set_counts = {'train': 0, 'val': 0, 'test': 0}\n",
        "\n",
        "        # Process each subtype separately to ensure stratification\n",
        "        for subtype in structured_data[cls]:\n",
        "            sn_objects = structured_data[cls][subtype]\n",
        "\n",
        "            # CRITICAL STEP: Sort by image count DESCENDING.\n",
        "            # This ensures \"Heavy\" SNs are processed first.\n",
        "            # Since Train has the highest target capacity, it naturally absorbs\n",
        "            # the large objects that would otherwise overflow Val/Test.\n",
        "            sn_objects.sort(key=lambda x: x['count'], reverse=True)\n",
        "\n",
        "            total_subtype_imgs = sum(x['count'] for x in sn_objects)\n",
        "\n",
        "            for sn_obj in sn_objects:\n",
        "                count = sn_obj['count']\n",
        "\n",
        "                # Calculate which set is most \"under-filled\" relative to target ratio\n",
        "                # Score = (Current Count + New Item Count) / Target Ratio\n",
        "                # We want the set that minimizes this 'fullness' score, or\n",
        "                # strictly fits the capacity.\n",
        "\n",
        "                best_set = None\n",
        "                best_score = float('inf')\n",
        "\n",
        "                total_current = sum(set_counts.values()) + count\n",
        "                if total_current == 0: total_current = 1 # Avoid div by zero\n",
        "\n",
        "                for s_name in ['train', 'val', 'test']:\n",
        "                    # Normalized fullness: How close are we to the target %?\n",
        "                    # A lower score means this set is \"hungrier\"\n",
        "                    current_pct = set_counts[s_name] / total_current\n",
        "                    target = ratios[s_name]\n",
        "\n",
        "                    # Score: Distance from target (prioritize keeping below target)\n",
        "                    score = (set_counts[s_name] + count) / target\n",
        "\n",
        "                    if score < best_score:\n",
        "                        best_score = score\n",
        "                        best_set = s_name\n",
        "\n",
        "                # Assign SN to the chosen set\n",
        "                final_split[cls][best_set].extend(sn_obj['images'])\n",
        "                set_counts[best_set] += count\n",
        "\n",
        "    return final_split\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "split0 = load_split(5)\n",
        "all_images = {'0': split0['0']['train']+split0['0']['val']+split0['0']['test'],\n",
        "              '1': split0['1']['train']+split0['1']['val']+split0['1']['test'],\n",
        "              '2': split0['2']['train']+split0['2']['val']+split0['2']['test']}"
      ],
      "metadata": {
        "id": "c1L_dUJeXQEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total images in augmented_dataset_v2.0\n",
        "total = []\n",
        "for key, value in all_images.items():\n",
        "    total.append(len(value))\n",
        "print('total images:', sum(total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S067n6Qypf2D",
        "outputId": "5b787480-e5f5-409e-90ff-da66f04b1193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total images: 7159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many original images have copies?\n",
        "with_copies = []\n",
        "for key, value in all_images.items():\n",
        "  for image in value:\n",
        "    if \"COPY\" in image:\n",
        "      with_copies.append(image.split('_')[0])\n",
        "\n",
        "with_copies = set(with_copies)\n",
        "len(with_copies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSVKGdnxkllQ",
        "outputId": "012c2010-7f63-4200-bbd4-c4411c475704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many originals per-class?\n",
        "originals = {}\n",
        "for key, value in all_images.items():\n",
        "  originals[key] = [image for image in value if \"COPY\" not in image]\n",
        "\n",
        "total_originals = []\n",
        "for key, value in originals.items():\n",
        "    total_originals.append(len(value))\n",
        "    print(f'originals class {key}: {len(value)}')\n",
        "print('original images:',sum(total_originals))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9sn-9J_qORV",
        "outputId": "cfdb80ad-9cbe-4b8c-bb1e-a4628b0e2661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "originals class 0: 2387\n",
            "originals class 1: 901\n",
            "originals class 2: 1416\n",
            "original images: 4704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many originals have zero copies\n",
        "# This are the ones eligible for test-set\n",
        "originals_eligible = {}\n",
        "for key, value in all_images.items():\n",
        "  originals_eligible[key] = [image for image in value if image.split('_')[0] not in with_copies]\n",
        "\n",
        "total_originals_eligible = []\n",
        "for key, value in originals_eligible.items():\n",
        "    total_originals_eligible.append(len(value))\n",
        "    print(f'originals eligible class {key}: {len(value)}')\n",
        "print('original eligible images:',sum(total_originals_eligible))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCEr_GaSiDww",
        "outputId": "61ec3d3e-3f94-4e98-e7b0-cc34ca82c880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "originals eligible class 0: 2387\n",
            "originals eligible class 1: 62\n",
            "originals eligible class 2: 1084\n",
            "original eligible images: 3533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With our current eligible images its impossible to\n",
        "# split in test and validation without using copies in them\n",
        "c = 1\n",
        "subty = []\n",
        "for image in originals[f\"{c}\"]:\n",
        "  sn_name, subtype = get_sn_info(image)\n",
        "  subty.append(subtype)\n",
        "subty = np.array(subty)\n",
        "print(np.unique(subty, return_counts=True))\n",
        "\n",
        "subty = []\n",
        "for image in originals_eligible[f\"{c}\"]:\n",
        "  sn_name, subtype = get_sn_info(image)\n",
        "  subty.append(subtype)\n",
        "subty = np.array(subty)\n",
        "print(np.unique(subty, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HScUuVCoga9-",
        "outputId": "d572e322-0998-4293-de7d-d0f0c9061cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([1, 2, 3, 4, 5]), array([398, 264,  30,  68, 141]))\n",
            "(array([1, 2]), array([51, 11]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# However, the test set should note include augmented data. A\n",
        "# And it must represent the original distribution.\n",
        "# We will keep the class imbalance in test/val\n",
        "# And training will be balanced via data augmentation.\n",
        "# Then we can drop copies that we will not have to use in train,\n",
        "# This will give more eligible images for test and val splits.\n",
        "\n",
        "# After doing numbers (using ratios 0.8,0.1,0.1 from original images)\n",
        "# We must drop 294 copies (or spectra) from class 1 and 193 from class 2\n",
        "# that is 58xsubtype for class 1 and 17xsubtype for class 2"
      ],
      "metadata": {
        "id": "8UIJGdjpxjt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To get subtype for a given basename\n",
        "def get_subtype_from_basename(basename):\n",
        "  for key, value in all_images.items():\n",
        "    for image in value:\n",
        "      if image.split('_')[0] == basename:\n",
        "        return int(image.split('_')[1])\n",
        "        break"
      ],
      "metadata": {
        "id": "1RCC4IMxFEvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the basenames that have copies for each class\n",
        "copies = {'0': [], '1': [], '2': []}\n",
        "for key, value in reduced_all_images.items():\n",
        "  for image in value:\n",
        "    if 'COPY' in image:\n",
        "      copies[key].append(image.split('_')[0])\n",
        "\n",
        "  copies[key] = set(copies[key])\n",
        "\n",
        "# For a given basename that has copies, give number of originals and copies\n",
        "def originals_and_copies(basename, dataset):\n",
        "  copies = 0\n",
        "  orig = 0\n",
        "  for key, value in dataset.items():\n",
        "    for image in value:\n",
        "      base = image.split('_')[0]\n",
        "      if base == basename:\n",
        "        if \"COPY\" in image:\n",
        "          copies = copies + 1\n",
        "        else:\n",
        "          orig = orig + 1\n",
        "  subtype = get_subtype_from_basename(basename)\n",
        "  print(f\"SN {basename} (subtype {subtype}) has {orig} originals and {copies} copies\")\n",
        "  return orig, copies, subtype, basename"
      ],
      "metadata": {
        "id": "CFwBCt4SAlEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data from filenames\n",
        "def parse_filename(fname):\n",
        "    parts = fname.replace('.png', '').split('_')\n",
        "    sn = parts[0]\n",
        "    subtype = int(parts[1])\n",
        "    is_copy = fname.endswith('_COPY.png')\n",
        "    return sn, subtype, is_copy\n"
      ],
      "metadata": {
        "id": "cbQXS4l0KuQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# Reduce dataset size to new constraints\n",
        "def drop_images_evenly(images, n_drop):\n",
        "    \"\"\"\n",
        "    images: list of filenames (same class & subtype)\n",
        "    n_drop: total images to drop\n",
        "    \"\"\"\n",
        "\n",
        "    # Group images by SN\n",
        "    sn_groups = defaultdict(list)\n",
        "    for img in images:\n",
        "        sn, _, _ = parse_filename(img)\n",
        "        sn_groups[sn].append(img)\n",
        "\n",
        "    # Separate COPY and ORIGINAL images per SN\n",
        "    sn_copies = {\n",
        "        sn: [img for img in imgs if img.endswith('_COPY.png')]\n",
        "        for sn, imgs in sn_groups.items()\n",
        "    }\n",
        "\n",
        "    dropped = set()\n",
        "\n",
        "    # Round-robin removal from COPY images\n",
        "    sn_list = list(sn_groups.keys())\n",
        "    idx = 0\n",
        "\n",
        "    while len(dropped) < n_drop:\n",
        "        sn = sn_list[idx % len(sn_list)]\n",
        "        if sn_copies[sn]:\n",
        "            dropped.add(sn_copies[sn].pop())\n",
        "        idx += 1\n",
        "\n",
        "        # Stop if no COPY images remain anywhere\n",
        "        if all(len(v) == 0 for v in sn_copies.values()):\n",
        "            break\n",
        "\n",
        "    # If still need to drop, fall back to originals (last resort)\n",
        "    if len(dropped) < n_drop:\n",
        "        remaining = [\n",
        "            img for img in images\n",
        "            if img not in dropped\n",
        "        ]\n",
        "        needed = n_drop - len(dropped)\n",
        "        dropped.update(random.sample(remaining, needed))\n",
        "\n",
        "    return dropped\n",
        "\n",
        "def build_reduced_dataset(all_images):\n",
        "    reduced = {}\n",
        "\n",
        "    # Drop rules\n",
        "    drop_rules = {\n",
        "        '1': 58,\n",
        "        '2': 17\n",
        "    }\n",
        "\n",
        "    for cls, images in all_images.items():\n",
        "        if cls not in drop_rules:\n",
        "            reduced[cls] = images.copy()\n",
        "            continue\n",
        "\n",
        "        # Group by subtype\n",
        "        by_subtype = defaultdict(list)\n",
        "        for img in images:\n",
        "            _, subtype, _ = parse_filename(img)\n",
        "            by_subtype[subtype].append(img)\n",
        "\n",
        "        to_drop = set()\n",
        "\n",
        "        for subtype, imgs in by_subtype.items():\n",
        "            dropped = drop_images_evenly(imgs, drop_rules[cls])\n",
        "            to_drop.update(dropped)\n",
        "\n",
        "        reduced[cls] = [img for img in images if img not in to_drop]\n",
        "\n",
        "    return reduced\n"
      ],
      "metadata": {
        "id": "s3dtwwHAK3mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_all_images = build_reduced_dataset(all_images)"
      ],
      "metadata": {
        "id": "4qmIvI1UK6gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check the dropped images\n",
        "def check_drops(original, reduced, cls):\n",
        "    from collections import Counter\n",
        "    o = Counter(parse_filename(i)[1] for i in original[cls])\n",
        "    r = Counter(parse_filename(i)[1] for i in reduced[cls])\n",
        "\n",
        "    for st in sorted(o):\n",
        "        print(f\"class {cls}, subtype {st}: dropped = {o[st] - r[st]}\")\n"
      ],
      "metadata": {
        "id": "jVU2HaT6K-45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_drops(all_images, reduced_all_images, '1')\n",
        "check_drops(all_images, reduced_all_images, '2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvHN-I1jLA3L",
        "outputId": "24fe9778-86d5-439a-adef-da0f5943ee57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class 1, subtype 1: dropped = 58\n",
            "class 1, subtype 2: dropped = 58\n",
            "class 1, subtype 3: dropped = 59\n",
            "class 1, subtype 4: dropped = 58\n",
            "class 1, subtype 5: dropped = 58\n",
            "class 2, subtype 6: dropped = 17\n",
            "class 2, subtype 7: dropped = 17\n",
            "class 2, subtype 8: dropped = 17\n",
            "class 2, subtype 9: dropped = 17\n",
            "class 2, subtype 10: dropped = 17\n",
            "class 2, subtype 11: dropped = 17\n",
            "class 2, subtype 12: dropped = 17\n",
            "class 2, subtype 13: dropped = 17\n",
            "class 2, subtype 14: dropped = 17\n",
            "class 2, subtype 15: dropped = 17\n",
            "class 2, subtype 16: dropped = 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Which images have copies now?\n",
        "copies2 = {'0': [], '1': [], '2': []}\n",
        "for key, value in reduced_all_images.items():\n",
        "  for image in value:\n",
        "    if 'COPY' in image:\n",
        "      copies2[key].append(image.split('_')[0])\n",
        "\n",
        "  copies2[key] = set(copies2[key])"
      ],
      "metadata": {
        "id": "ou5QRdwxOnR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_copies2 = []\n",
        "for key, value in reduced_all_images.items():\n",
        "  for image in value:\n",
        "    if \"COPY\" in image:\n",
        "      with_copies2.append(image.split('_')[0])\n",
        "\n",
        "with_copies2 = set(with_copies2)\n",
        "len(with_copies2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2pUpL63L7Of",
        "outputId": "401ba0f3-3aac-4512-a98f-f2c4bcb0105d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 427
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original images updated to reduced dataset\n",
        "originals2 = {}\n",
        "for key, value in reduced_all_images.items():\n",
        "  originals2[key] = [image for image in value if \"COPY\" not in image]\n",
        "\n",
        "total_originals2 = []\n",
        "for key, value in originals2.items():\n",
        "    total_originals2.append(len(value))\n",
        "    print(f'originals class {key}: {len(value)}')\n",
        "print('original images:',sum(total_originals2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdJ3lVD_MSsN",
        "outputId": "725a86f4-e21f-45f0-a238-243cb8b85823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "originals class 0: 2387\n",
            "originals class 1: 901\n",
            "originals class 2: 1331\n",
            "original images: 4619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Eligible for val/test updated to reduced dataset\n",
        "originals_eligible2 = {}\n",
        "for key, value in reduced_all_images.items():\n",
        "  originals_eligible2[key] = [image for image in value if image.split('_')[0] not in with_copies2]\n",
        "\n",
        "total_originals_eligible2 = []\n",
        "for key, value in originals_eligible2.items():\n",
        "    total_originals_eligible2.append(len(value))\n",
        "    print(f'originals eligible class {key}: {len(value)}')\n",
        "print('original eligible images:',sum(total_originals_eligible2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YL_dLPWLCf9",
        "outputId": "f3bffdd2-769f-4923-e239-8bb8f25038f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "originals eligible class 0: 2387\n",
            "originals eligible class 1: 262\n",
            "originals eligible class 2: 1031\n",
            "original eligible images: 3680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For class c, what is the subtype representation in originals2 and eligible originals2\n",
        "c = 1\n",
        "subty = []\n",
        "for image in originals2[f\"{c}\"]:\n",
        "  sn_name, subtype = get_sn_info(image)\n",
        "  subty.append(subtype)\n",
        "subty = np.array(subty)\n",
        "print(np.unique(subty, return_counts=True))\n",
        "\n",
        "subty = []\n",
        "for image in originals_eligible2[f\"{c}\"]:\n",
        "  sn_name, subtype = get_sn_info(image)\n",
        "  subty.append(subtype)\n",
        "subty = np.array(subty)\n",
        "print(np.unique(subty, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhFy8BQyLR5W",
        "outputId": "e70edfdd-15eb-446b-ab06-4c483d857f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([1, 2, 3, 4, 5]), array([398, 264,  30,  68, 141]))\n",
            "(array([1, 2, 5]), array([235,  26,   1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for basename in copies2['1']:\n",
        "  orig, copies, subtype = originals_and_copies(basename, reduced_all_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kSER7uBOYN1",
        "outputId": "b5e723ba-9aac-4cc6-916a-9a025846f537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SN sn02cx (subtype 4) has 8 originals and 73 copies\n",
            "SN sn2002dl (subtype 2) has 4 originals and 1 copies\n",
            "SN sn02fb (subtype 2) has 2 originals and 2 copies\n",
            "SN sn06ke (subtype 2) has 1 originals and 1 copies\n",
            "SN sn2005gj (subtype 3) has 22 originals and 182 copies\n",
            "SN sn91T (subtype 1) has 21 originals and 1 copies\n",
            "SN sn2000cn (subtype 2) has 10 originals and 4 copies\n",
            "SN sn2008ae (subtype 5) has 5 originals and 13 copies\n",
            "SN sn99by (subtype 2) has 15 originals and 12 copies\n",
            "SN sn00cx (subtype 5) has 26 originals and 82 copies\n",
            "SN sn05ke (subtype 2) has 3 originals and 1 copies\n",
            "SN sn1998es (subtype 1) has 26 originals and 1 copies\n",
            "SN sn2005ke (subtype 2) has 10 originals and 1 copies\n",
            "SN sn2006hb (subtype 2) has 8 originals and 4 copies\n",
            "SN sn03gq (subtype 4) has 1 originals and 4 copies\n",
            "SN sn2003Y (subtype 2) has 3 originals and 2 copies\n",
            "SN sn2008A (subtype 4) has 14 originals and 133 copies\n",
            "SN sn2006oa (subtype 1) has 7 originals and 1 copies\n",
            "SN sn2007al (subtype 2) has 8 originals and 1 copies\n",
            "SN sn2005hj (subtype 1) has 10 originals and 2 copies\n",
            "SN sn2007if (subtype 5) has 38 originals and 107 copies\n",
            "SN sn2006gz (subtype 5) has 17 originals and 35 copies\n",
            "SN sn2005hk (subtype 4) has 32 originals and 85 copies\n",
            "SN sn1999by (subtype 2) has 14 originals and 2 copies\n",
            "SN sn05gj (subtype 3) has 3 originals and 74 copies\n",
            "SN sn2001V (subtype 1) has 38 originals and 7 copies\n",
            "SN sn1986G (subtype 2) has 28 originals and 27 copies\n",
            "SN sn2007ax (subtype 2) has 9 originals and 9 copies\n",
            "SN sn2006H (subtype 2) has 14 originals and 13 copies\n",
            "SN sn1995bd (subtype 1) has 8 originals and 1 copies\n",
            "SN sn1998de (subtype 2) has 7 originals and 9 copies\n",
            "SN sn2002ic (subtype 3) has 5 originals and 133 copies\n",
            "SN sn08A (subtype 4) has 1 originals and 9 copies\n",
            "SN sn1999ac (subtype 1) has 34 originals and 6 copies\n",
            "SN sn2000cx (subtype 5) has 44 originals and 19 copies\n",
            "SN sn05hk (subtype 4) has 4 originals and 31 copies\n",
            "SN sn2005cc (subtype 5) has 10 originals and 22 copies\n",
            "SN sn1997cn (subtype 2) has 13 originals and 11 copies\n",
            "SN sn1991bg (subtype 2) has 26 originals and 8 copies\n",
            "SN sn03Y (subtype 2) has 2 originals and 1 copies\n",
            "SN sn02cf (subtype 2) has 1 originals and 1 copies\n",
            "SN sn2005bl (subtype 2) has 8 originals and 5 copies\n",
            "SN sn2005mz (subtype 2) has 4 originals and 2 copies\n",
            "SN sn2003fa (subtype 1) has 19 originals and 2 copies\n",
            "SN sn2002es (subtype 2) has 6 originals and 8 copies\n",
            "SN sn91bg (subtype 2) has 32 originals and 26 copies\n",
            "SN sn2002fb (subtype 2) has 7 originals and 2 copies\n",
            "SN sn2002cx (subtype 4) has 8 originals and 16 copies\n",
            "SN sn07ba (subtype 2) has 3 originals and 2 copies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def group_by_sn(images):\n",
        "    sn_groups = defaultdict(lambda: {\n",
        "        'images': [],\n",
        "        'subtype': None\n",
        "    })\n",
        "\n",
        "    for img in images:\n",
        "        sn, subtype = get_sn_info(img)\n",
        "        if sn is None:\n",
        "            continue\n",
        "        sn_groups[sn]['images'].append(img)\n",
        "        sn_groups[sn]['subtype'] = subtype\n",
        "\n",
        "    return sn_groups\n"
      ],
      "metadata": {
        "id": "WmKjrPQWk5Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Function to select the test set first from eligible2\n",
        "def select_test_set_fast(\n",
        "    images,\n",
        "    target_images,\n",
        "    target_sns,\n",
        "    required_subtypes=None,\n",
        "    max_len_sn=0.2,\n",
        "    img_tol=5,\n",
        "    sn_tol=3,\n",
        "    max_tries=2000,\n",
        "    seed=42\n",
        "):\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Group by SN\n",
        "    sn_groups = defaultdict(lambda: {'images': [], 'subtype': None})\n",
        "    for img in images:\n",
        "        sn, subtype = get_sn_info(img)\n",
        "        if sn is None:\n",
        "            continue\n",
        "        sn_groups[sn]['images'].append(img)\n",
        "        sn_groups[sn]['subtype'] = subtype\n",
        "\n",
        "    # Filter by subtype\n",
        "    if required_subtypes is not None:\n",
        "        sn_groups = {\n",
        "            sn: d for sn, d in sn_groups.items()\n",
        "            if d['subtype'] in required_subtypes\n",
        "        }\n",
        "\n",
        "    sn_items = list(sn_groups.items())\n",
        "\n",
        "    best = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    for _ in range(max_tries):\n",
        "        random.shuffle(sn_items)\n",
        "\n",
        "        chosen = []\n",
        "        img_sum = 0\n",
        "\n",
        "        for sn, data in sn_items:\n",
        "            n = len(data['images'])\n",
        "            if img_sum + n <= target_images + img_tol:\n",
        "                if n < max_len_sn*target_images:\n",
        "                  chosen.append(sn)\n",
        "                  img_sum += n\n",
        "\n",
        "        sn_count = len(chosen)\n",
        "        img_error = abs(img_sum - target_images)\n",
        "        sn_error = abs(sn_count - target_sns)\n",
        "\n",
        "        score = img_error * 10 + sn_error  # prioritize image count\n",
        "\n",
        "        if img_error <= img_tol and sn_error <= sn_tol:\n",
        "            best = chosen\n",
        "            break\n",
        "\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best = chosen\n",
        "\n",
        "    # Flatten images\n",
        "    test_images = []\n",
        "    for sn in best:\n",
        "        test_images.extend(sn_groups[sn]['images'])\n",
        "\n",
        "    return test_images\n",
        "\n"
      ],
      "metadata": {
        "id": "zlh604GUp0P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_set(seed):\n",
        "  test_set = {}\n",
        "\n",
        "  test_set['0'] = select_test_set_fast(\n",
        "      originals_eligible2['0'],\n",
        "      target_images=238,\n",
        "      target_sns=30,\n",
        "      max_len_sn=0.2,\n",
        "      img_tol=5,\n",
        "      sn_tol=4,\n",
        "      seed=seed+10\n",
        "  )\n",
        "\n",
        "  test_set['1'] = select_test_set_fast(\n",
        "      originals_eligible2['1'],\n",
        "      target_images=90,\n",
        "      target_sns=9,\n",
        "      max_len_sn=0.3,\n",
        "      required_subtypes={1, 2, 5},\n",
        "      img_tol=4,\n",
        "      sn_tol=2,\n",
        "      seed=seed+20\n",
        "  )\n",
        "\n",
        "  test_set['2'] = select_test_set_fast(\n",
        "      originals_eligible2['2'],\n",
        "      target_images=141,\n",
        "      target_sns=10,\n",
        "      max_len_sn=0.2,\n",
        "      required_subtypes={6, 8, 10, 11, 13, 16},\n",
        "      img_tol=5,\n",
        "      sn_tol=3,\n",
        "      seed=seed+30\n",
        "  )\n",
        "\n",
        "  return test_set"
      ],
      "metadata": {
        "id": "iyfuCuphp1pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_test_set(test_set):\n",
        "    subt = {'0': [], '1': [], '2': []}\n",
        "    for cls, imgs in test_set.items():\n",
        "        sns = set()\n",
        "        subtypes = set()\n",
        "        for img in imgs:\n",
        "            sn, st = get_sn_info(img)\n",
        "            sns.add(sn)\n",
        "            subtypes.add(st)\n",
        "        print(f\"\\nClass {cls}\")\n",
        "        print(\" images:\", len(imgs))\n",
        "        print(\" unique SNs:\", len(sns))\n",
        "        print(\" subtypes:\", sorted(subtypes))\n",
        "        subt[cls].append(len(subtypes))\n",
        "    return subt"
      ],
      "metadata": {
        "id": "TmzgJatzq8vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=0\n",
        "while True:\n",
        "  test_set2 = create_test_set(seed=145+k)\n",
        "  subt = check_test_set(test_set2)\n",
        "  # print(subt)\n",
        "  if subt['2'][0] == 5 and subt['1'][0] == 3: # ensure the maximum subtype representation possible\n",
        "    break\n",
        "  k = k + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRMJaJjOp42I",
        "outputId": "678d7ce3-3366-4e3a-a7f0-6f68bee8f6c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class 0\n",
            " images: 243\n",
            " unique SNs: 31\n",
            " subtypes: [0]\n",
            "\n",
            "Class 1\n",
            " images: 94\n",
            " unique SNs: 11\n",
            " subtypes: [1, 2]\n",
            "\n",
            "Class 2\n",
            " images: 146\n",
            " unique SNs: 13\n",
            " subtypes: [6, 8, 10, 11, 13]\n",
            "\n",
            "Class 0\n",
            " images: 243\n",
            " unique SNs: 34\n",
            " subtypes: [0]\n",
            "\n",
            "Class 1\n",
            " images: 94\n",
            " unique SNs: 10\n",
            " subtypes: [1, 2]\n",
            "\n",
            "Class 2\n",
            " images: 146\n",
            " unique SNs: 13\n",
            " subtypes: [6, 8, 10, 11, 13]\n",
            "\n",
            "Class 0\n",
            " images: 243\n",
            " unique SNs: 31\n",
            " subtypes: [0]\n",
            "\n",
            "Class 1\n",
            " images: 94\n",
            " unique SNs: 10\n",
            " subtypes: [1, 2, 5]\n",
            "\n",
            "Class 2\n",
            " images: 146\n",
            " unique SNs: 12\n",
            " subtypes: [6, 8, 10, 11, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_test_set(test_set2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F2AQJ1I-62X",
        "outputId": "e20649b0-e0d3-486e-b541-d2e0cd51de7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class 0\n",
            " images: 243\n",
            " unique SNs: 31\n",
            " subtypes: [0]\n",
            "\n",
            "Class 1\n",
            " images: 94\n",
            " unique SNs: 10\n",
            " subtypes: [1, 2, 5]\n",
            "\n",
            "Class 2\n",
            " images: 146\n",
            " unique SNs: 12\n",
            " subtypes: [6, 8, 10, 11, 13]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': [1], '1': [3], '2': [5]}"
            ]
          },
          "metadata": {},
          "execution_count": 478
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def images_per_sn(split_dict):\n",
        "    \"\"\"\n",
        "    split_dict: dict with keys ['0','1','2'] and values = list of image names\n",
        "    \"\"\"\n",
        "    summary = {}\n",
        "\n",
        "    for cls, images in split_dict.items():\n",
        "        sn_counter = Counter()\n",
        "\n",
        "        for img in images:\n",
        "            sn, _ = get_sn_info(img)\n",
        "            if sn is not None:\n",
        "                sn_counter[sn] += 1\n",
        "\n",
        "        summary[cls] = sn_counter\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "0-Jlfh5VsAc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_images_per_sn(sn_summary, top_n=10):\n",
        "    for cls, counter in sn_summary.items():\n",
        "        print(f\"\\nClass {cls}\")\n",
        "        print(f\" unique SNs: {len(counter)}\")\n",
        "        print(f\" total images: {sum(counter.values())}\")\n",
        "\n",
        "        counts = list(counter.values())\n",
        "        print(f\" images per SN: min={min(counts)}, \"\n",
        "              f\"max={max(counts)}, \"\n",
        "              f\"mean={sum(counts)/len(counts):.2f}\")\n",
        "\n",
        "        print(\" top SNs by image count:\")\n",
        "        for sn, n in counter.most_common(top_n):\n",
        "            print(f\"   {sn}: {n}\")"
      ],
      "metadata": {
        "id": "RrHewjq4sTfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sn_summary = images_per_sn(test_set2)\n",
        "print_images_per_sn(sn_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAuDe41BsZcS",
        "outputId": "631e2105-f46f-4dbf-b37e-50930cd1009a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class 0\n",
            " unique SNs: 31\n",
            " total images: 243\n",
            " images per SN: min=1, max=26, mean=7.84\n",
            " top SNs by image count:\n",
            "   sn2002er: 26\n",
            "   sn2003kf: 26\n",
            "   sn89B: 24\n",
            "   sn2002dj: 23\n",
            "   sn2004eo: 19\n",
            "   sn1994ae: 18\n",
            "   sn2003W: 16\n",
            "   sn2000fa: 15\n",
            "   sn2001fe: 12\n",
            "   sn2001N: 8\n",
            "\n",
            "Class 1\n",
            " unique SNs: 10\n",
            " total images: 94\n",
            " images per SN: min=1, max=21, mean=9.40\n",
            " top SNs by image count:\n",
            "   sn1997br: 21\n",
            "   sn1999dq: 19\n",
            "   sn2001eh: 18\n",
            "   sn1998ab: 12\n",
            "   sn08ds: 9\n",
            "   sn2002hu: 5\n",
            "   sn2003hu: 4\n",
            "   sn99da: 3\n",
            "   sn1999cw: 2\n",
            "   snls03D3bb: 1\n",
            "\n",
            "Class 2\n",
            " unique SNs: 12\n",
            " total images: 146\n",
            " images per SN: min=1, max=26, mean=12.17\n",
            " top SNs by image count:\n",
            "   13ge: 26\n",
            "   sn2005bf: 23\n",
            "   sn2006bp: 21\n",
            "   sn98bw: 13\n",
            "   sn2007gr: 13\n",
            "   sn94I: 12\n",
            "   sn2003bg: 11\n",
            "   sn2004gt: 10\n",
            "   sn2011ei: 8\n",
            "   sn2011fu: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_images(all_images, images_to_drop):\n",
        "    remaining = {}\n",
        "    for cls, imgs in all_images.items():\n",
        "        drop_set = set(images_to_drop.get(cls, []))\n",
        "        remaining[cls] = [img for img in imgs if img not in drop_set]\n",
        "\n",
        "    return remaining"
      ],
      "metadata": {
        "id": "o0iGPSTmsZ1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the test set images so that we can now select the validation sets\n",
        "remaining_images = drop_images(originals_eligible2, test_set2)"
      ],
      "metadata": {
        "id": "-D9VgjcBthJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(remaining_images['1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whRrPhYxtirC",
        "outputId": "9c6346b0-4513-46eb-8aa0-2b6d062ca450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "169"
            ]
          },
          "metadata": {},
          "execution_count": 513
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to see the overlap between two validation sets\n",
        "def overlap_fraction(set_a, set_b):\n",
        "    a = set(set_a)\n",
        "    b = set(set_b)\n",
        "    if len(a) == 0:\n",
        "        return 0.0\n",
        "    return len(a & b) / len(a)"
      ],
      "metadata": {
        "id": "XBRItb73uKq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 4 validation sets\n",
        "# We put a threshold to how much they can overlap (0.3)\n",
        "# And also ensure that a single SN can't have more than 20% of the split\n",
        "while True:\n",
        "    val_sets = []\n",
        "\n",
        "    for k in range(4):\n",
        "        vs = {}\n",
        "\n",
        "        vs['0'] = select_test_set_fast(\n",
        "            remaining_images['0'],\n",
        "            target_images=238,\n",
        "            target_sns=30,\n",
        "            max_len_sn=0.2,\n",
        "            img_tol=5,\n",
        "            sn_tol=3,\n",
        "            seed=32 + seed_sum + k\n",
        "        )\n",
        "\n",
        "        vs['1'] = select_test_set_fast(\n",
        "            remaining_images['1'],\n",
        "            target_images=90,\n",
        "            target_sns=9,\n",
        "            max_len_sn=0.25,\n",
        "            required_subtypes={1, 2, 4, 5},\n",
        "            img_tol=5,\n",
        "            sn_tol=3,\n",
        "            seed=32 + seed_sum + k\n",
        "        )\n",
        "\n",
        "        vs['2'] = select_test_set_fast(\n",
        "            remaining_images['2'],\n",
        "            target_images=141,\n",
        "            target_sns=10,\n",
        "            max_len_sn=0.2,\n",
        "            required_subtypes={6, 8, 10, 11, 13, 16},\n",
        "            img_tol=5,\n",
        "            sn_tol=3,\n",
        "            seed=32 + seed_sum + k\n",
        "        )\n",
        "\n",
        "        val_sets.append(vs)\n",
        "\n",
        "    thresh = 0.3\n",
        "    ok = True\n",
        "\n",
        "    for cls in ['0', '1', '2']:\n",
        "        for i in range(4):\n",
        "            for j in range(i + 1, 4):\n",
        "                if overlap_fraction(val_sets[i][cls], val_sets[j][cls]) > thresh:\n",
        "                    ok = False\n",
        "                    break\n",
        "            if not ok:\n",
        "                break\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "    if ok:\n",
        "        break\n",
        "\n",
        "    seed_sum += 1"
      ],
      "metadata": {
        "id": "y8d0fr9CwYjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "84b6b9e2-4e33-47ab-acf0-f46fe5772415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1035827716.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         vs['2'] = select_test_set_fast(\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mremaining_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtarget_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m141\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3254132251.py\u001b[0m in \u001b[0;36mselect_test_set_fast\u001b[0;34m(images, target_images, target_sns, required_subtypes, max_len_sn, img_tol, sn_tol, max_tries, seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msn_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mimg_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtarget_images\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_tol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_len_sn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtarget_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                   \u001b[0mchosen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_set1 = val_sets[0]\n",
        "val_set2 = val_sets[1]\n",
        "val_set3 = val_sets[2]\n",
        "val_set4 = val_sets[3]"
      ],
      "metadata": {
        "id": "WXQJY1fFzhue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check overlap\n",
        "for cls in ['0', '1', '2']:\n",
        "  print(overlap_fraction(val_set1[cls], val_set2[cls]))\n",
        "  print(overlap_fraction(val_set1[cls], val_set3[cls]))\n",
        "  print(overlap_fraction(val_set1[cls], val_set4[cls]))\n",
        "  print(overlap_fraction(val_set2[cls], val_set3[cls]))\n",
        "  print(overlap_fraction(val_set2[cls], val_set4[cls]))\n",
        "  print(overlap_fraction(val_set3[cls], val_set4[cls]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mynPD80WuwJL",
        "outputId": "b94abedd-8a06-4e0a-a586-b727dec0fbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0411522633744856\n",
            "0.18106995884773663\n",
            "0.0823045267489712\n",
            "0.07407407407407407\n",
            "0.32510288065843623\n",
            "0.024691358024691357\n",
            "0.3157894736842105\n",
            "0.10526315789473684\n",
            "0.3157894736842105\n",
            "0.28421052631578947\n",
            "0.2631578947368421\n",
            "0.25263157894736843\n",
            "0.1232876712328767\n",
            "0.23972602739726026\n",
            "0.273972602739726\n",
            "0.3356164383561644\n",
            "0.2876712328767123\n",
            "0.273972602739726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sn_summary = images_per_sn(test_set2)\n",
        "print_images_per_sn(sn_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK_CfgRo0PAT",
        "outputId": "d61b2d1b-7dff-4f58-d07d-51d21e1e4a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class 0\n",
            " unique SNs: 31\n",
            " total images: 243\n",
            " images per SN: min=1, max=26, mean=7.84\n",
            " top SNs by image count:\n",
            "   sn2002er: 26\n",
            "   sn2003kf: 26\n",
            "   sn89B: 24\n",
            "   sn2002dj: 23\n",
            "   sn2004eo: 19\n",
            "   sn1994ae: 18\n",
            "   sn2003W: 16\n",
            "   sn2000fa: 15\n",
            "   sn2001fe: 12\n",
            "   sn2001N: 8\n",
            "\n",
            "Class 1\n",
            " unique SNs: 13\n",
            " total images: 93\n",
            " images per SN: min=1, max=21, mean=7.15\n",
            " top SNs by image count:\n",
            "   sn1997br: 21\n",
            "   sn1999dq: 19\n",
            "   sn2001eh: 18\n",
            "   sn08ds: 9\n",
            "   sn2002hu: 5\n",
            "   sn2003hu: 4\n",
            "   sn99da: 3\n",
            "   sn2006gt: 3\n",
            "   sn2006bz: 3\n",
            "   sn2007ba: 3\n",
            "\n",
            "Class 2\n",
            " unique SNs: 12\n",
            " total images: 146\n",
            " images per SN: min=1, max=26, mean=12.17\n",
            " top SNs by image count:\n",
            "   13ge: 26\n",
            "   sn2005bf: 23\n",
            "   sn2006bp: 21\n",
            "   sn98bw: 13\n",
            "   sn2007gr: 13\n",
            "   sn94I: 12\n",
            "   sn2003bg: 11\n",
            "   sn2004gt: 10\n",
            "   sn2011ei: 8\n",
            "   sn2011fu: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_test_set(val_set1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q02uLdHLENe6",
        "outputId": "36d45bfb-a949-49e5-84e9-dfaffb89a742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class 0\n",
            " images: 243\n",
            " unique SNs: 29\n",
            " subtypes: [0]\n",
            "\n",
            "Class 1\n",
            " images: 94\n",
            " unique SNs: 11\n",
            " subtypes: [1, 2]\n",
            "\n",
            "Class 2\n",
            " images: 146\n",
            " unique SNs: 13\n",
            " subtypes: [6, 8, 10, 11, 13]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': [1], '1': [2], '2': [5]}"
            ]
          },
          "metadata": {},
          "execution_count": 615
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See what remains for validation sets..\n",
        "c = 1\n",
        "\n",
        "subty = []\n",
        "for image in remaining_images[f\"{c}\"]:\n",
        "  sn_name, subtype = get_sn_info(image)\n",
        "  subty.append(subtype)\n",
        "subty = np.array(subty)\n",
        "print(np.unique(subty, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KiSEWJxKMIK",
        "outputId": "95103a72-e4df-4dc3-9f86-a81a1c3944e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([1, 2, 4, 5]), array([305, 186,  13,   5]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# See what remains for validation sets..\n",
        "by_sn = defaultdict(lambda: {'subtypes': set(), 'n_images': 0})\n",
        "\n",
        "for img in remaining_images['2']:\n",
        "    sn, subtype = get_sn_info(img)\n",
        "    if sn is None:\n",
        "        continue\n",
        "    by_sn[sn]['subtypes'].add(subtype)\n",
        "    by_sn[sn]['n_images'] += 1\n",
        "\n",
        "# Print summary\n",
        "print(f\"{'SN name':<20} | {'#images':<8} | subtypes\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "for sn in sorted(by_sn):\n",
        "    info = by_sn[sn]\n",
        "    print(f\"{sn:<20} | {info['n_images']:<8} | {sorted(info['subtypes'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYHqyQe9fxen",
        "outputId": "5333c647-bfed-4346-f56c-226203cce725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SN name              | #images  | subtypes\n",
            "---------------------------------------------\n",
            "10as                 | 16       | [8]\n",
            "11hs                 | 15       | [8]\n",
            "12au                 | 6        | [6]\n",
            "15dtg                | 7        | [10]\n",
            "16coi                | 13       | [11]\n",
            "16gkg                | 9        | [8]\n",
            "17ein                | 5        | [10]\n",
            "LSQ14efd             | 14       | [10]\n",
            "PTF10bzf             | 2        | [11]\n",
            "PTF10qts             | 6        | [11]\n",
            "PTF10vgv             | 4        | [11]\n",
            "PTF12gzk             | 6        | [10]\n",
            "iPTF13bvn            | 13       | [6]\n",
            "sn1983N              | 2        | [6]\n",
            "sn1983V              | 10       | [10]\n",
            "sn1984L              | 7        | [6]\n",
            "sn1987A              | 200      | [16]\n",
            "sn1990B              | 17       | [10]\n",
            "sn1990I              | 7        | [6]\n",
            "sn1990U              | 5        | [6]\n",
            "sn1992ar             | 1        | [10]\n",
            "sn1993J              | 47       | [8]\n",
            "sn1994I              | 30       | [10]\n",
            "sn1996cb             | 18       | [8]\n",
            "sn1997ef             | 19       | [11]\n",
            "sn1998bw             | 19       | [11]\n",
            "sn1998dt             | 7        | [6]\n",
            "sn1998fa             | 3        | [8]\n",
            "sn1999ex             | 2        | [6]\n",
            "sn1999gi             | 11       | [13]\n",
            "sn2000H              | 4        | [8]\n",
            "sn2002ap             | 30       | [11]\n",
            "sn2003dh             | 7        | [11]\n",
            "sn2003jd             | 18       | [11]\n",
            "sn2003lw             | 2        | [11]\n",
            "sn2004dk             | 3        | [6]\n",
            "sn2004dn             | 4        | [10]\n",
            "sn2004fe             | 8        | [10]\n",
            "sn2004ge             | 1        | [10]\n",
            "sn2004gq             | 19       | [6]\n",
            "sn2004gv             | 1        | [6]\n",
            "sn2005az             | 19       | [10]\n",
            "sn2005hg             | 14       | [6]\n",
            "sn2005kl             | 2        | [10]\n",
            "sn2005mf             | 3        | [10]\n",
            "sn2006T              | 4        | [8]\n",
            "sn2006aj             | 15       | [11]\n",
            "sn2006el             | 5        | [8]\n",
            "sn2006ep             | 3        | [6]\n",
            "sn2007C              | 14       | [6]\n",
            "sn2007D              | 1        | [11]\n",
            "sn2007Y              | 10       | [6]\n",
            "sn2007bg             | 3        | [11]\n",
            "sn2007cl             | 1        | [10]\n",
            "sn2007ru             | 9        | [11]\n",
            "sn2008D              | 13       | [6]\n",
            "sn2008ax             | 16       | [8]\n",
            "sn2008bo             | 7        | [8]\n",
            "sn2009bb             | 9        | [11]\n",
            "sn2009er             | 8        | [6]\n",
            "sn2009iz             | 6        | [6]\n",
            "sn2009jf             | 34       | [6]\n",
            "sn2009mg             | 7        | [8]\n",
            "sn2009nz             | 1        | [11]\n",
            "sn2010ma             | 2        | [11]\n",
            "sn2011bm             | 10       | [10]\n",
            "sn2011dh             | 15       | [8]\n",
            "sn2012P              | 4        | [8]\n",
            "sn2012ap             | 10       | [11]\n",
            "sn2012bz             | 2        | [11]\n",
            "sn2013cq             | 1        | [11]\n",
            "sn2013df             | 5        | [8]\n",
            "sn2013dk             | 1        | [10]\n",
            "sn2013dx             | 10       | [11]\n",
            "sn2014ad             | 3        | [11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_add_c1 = []\n",
        "for basename in copies['1']:\n",
        "  orig, copies_, subtype, base = originals_and_copies(basename, reduced_all_images)\n",
        "  if copies_ + orig < 18:\n",
        "    to_add_c1.append(basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCA0Vy5-rDUV",
        "outputId": "d5bfba12-4988-4ffe-ea5a-f8c1bafc9d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SN sn02cx (subtype 4) has 8 originals and 73 copies\n",
            "SN sn2002dl (subtype 2) has 4 originals and 1 copies\n",
            "SN sn02fb (subtype 2) has 2 originals and 2 copies\n",
            "SN sn06ke (subtype 2) has 1 originals and 1 copies\n",
            "SN sn2005gj (subtype 3) has 22 originals and 182 copies\n",
            "SN sn91T (subtype 1) has 21 originals and 1 copies\n",
            "SN sn2000cn (subtype 2) has 10 originals and 4 copies\n",
            "SN sn2008ae (subtype 5) has 5 originals and 13 copies\n",
            "SN sn99by (subtype 2) has 15 originals and 12 copies\n",
            "SN sn00cx (subtype 5) has 26 originals and 82 copies\n",
            "SN sn05ke (subtype 2) has 3 originals and 1 copies\n",
            "SN sn1998es (subtype 1) has 26 originals and 1 copies\n",
            "SN sn2005ke (subtype 2) has 10 originals and 1 copies\n",
            "SN sn2006hb (subtype 2) has 8 originals and 4 copies\n",
            "SN sn03gq (subtype 4) has 1 originals and 4 copies\n",
            "SN sn2003Y (subtype 2) has 3 originals and 2 copies\n",
            "SN sn2008A (subtype 4) has 14 originals and 133 copies\n",
            "SN sn2006oa (subtype 1) has 7 originals and 1 copies\n",
            "SN sn2007al (subtype 2) has 8 originals and 1 copies\n",
            "SN sn2005hj (subtype 1) has 10 originals and 2 copies\n",
            "SN sn2007if (subtype 5) has 38 originals and 107 copies\n",
            "SN sn2006gz (subtype 5) has 17 originals and 35 copies\n",
            "SN sn2005hk (subtype 4) has 32 originals and 85 copies\n",
            "SN sn1999by (subtype 2) has 14 originals and 2 copies\n",
            "SN sn05gj (subtype 3) has 3 originals and 74 copies\n",
            "SN sn2001V (subtype 1) has 38 originals and 7 copies\n",
            "SN sn1986G (subtype 2) has 28 originals and 27 copies\n",
            "SN sn2007ax (subtype 2) has 9 originals and 9 copies\n",
            "SN sn2006H (subtype 2) has 14 originals and 13 copies\n",
            "SN sn1995bd (subtype 1) has 8 originals and 1 copies\n",
            "SN sn1998de (subtype 2) has 7 originals and 9 copies\n",
            "SN sn2002ic (subtype 3) has 5 originals and 133 copies\n",
            "SN sn08A (subtype 4) has 1 originals and 9 copies\n",
            "SN sn1999ac (subtype 1) has 34 originals and 6 copies\n",
            "SN sn2000cx (subtype 5) has 44 originals and 19 copies\n",
            "SN sn05hk (subtype 4) has 4 originals and 31 copies\n",
            "SN sn2005cc (subtype 5) has 10 originals and 22 copies\n",
            "SN sn1997cn (subtype 2) has 13 originals and 11 copies\n",
            "SN sn1991bg (subtype 2) has 26 originals and 8 copies\n",
            "SN sn03Y (subtype 2) has 2 originals and 1 copies\n",
            "SN sn02cf (subtype 2) has 1 originals and 1 copies\n",
            "SN sn2005bl (subtype 2) has 8 originals and 5 copies\n",
            "SN sn2005mz (subtype 2) has 4 originals and 2 copies\n",
            "SN sn2003fa (subtype 1) has 19 originals and 2 copies\n",
            "SN sn2002es (subtype 2) has 6 originals and 8 copies\n",
            "SN sn91bg (subtype 2) has 32 originals and 26 copies\n",
            "SN sn2002fb (subtype 2) has 7 originals and 2 copies\n",
            "SN sn2002cx (subtype 4) has 8 originals and 16 copies\n",
            "SN sn07ba (subtype 2) has 3 originals and 2 copies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can compromise having some copies in val sets to gain subtype representation\n",
        "to_add_c2 = []\n",
        "for basename in copies['2']:\n",
        "  orig, copies_, subtype, base = originals_and_copies(basename, reduced_all_images)\n",
        "  if copies_ + orig < 28:\n",
        "    to_add_c2.append(basename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mACD9FuBEozJ",
        "outputId": "6f781a34-cdcf-4323-dd7d-c662bebeaccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SN sn1980K (subtype 14) has 8 originals and 122 copies\n",
            "SN sn2002ao (subtype 7) has 4 originals and 16 copies\n",
            "SN sn2004aw (subtype 12) has 27 originals and 133 copies\n",
            "SN sn2005cs (subtype 13) has 41 originals and 10 copies\n",
            "SN sn1999em (subtype 13) has 45 originals and 7 copies\n",
            "SN sn1992H (subtype 13) has 10 originals and 3 copies\n",
            "SN sn1998S (subtype 15) has 57 originals and 129 copies\n",
            "SN sn1979C (subtype 14) has 5 originals and 65 copies\n",
            "SN sn2005la (subtype 9) has 5 originals and 47 copies\n",
            "SN sn2004et (subtype 13) has 48 originals and 4 copies\n",
            "SN sn2005ek (subtype 12) has 6 originals and 34 copies\n",
            "SN sn1996L (subtype 15) has 7 originals and 7 copies\n",
            "SN sn2006jc (subtype 7) has 22 originals and 113 copies\n",
            "SN sn2007uy (subtype 9) has 10 originals and 138 copies\n",
            "SN sn2000er (subtype 7) has 5 originals and 40 copies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Images with copies to include in the pool for validation\n",
        "move1 = []\n",
        "for image in reduced_all_images['1']:\n",
        "  sn_name, subtype = get_sn_info(image)\n",
        "  if sn_name in to_add_c1:\n",
        "    move1.append(image)\n",
        "\n",
        "move2 = []\n",
        "for image in reduced_all_images['2']:\n",
        "  sn_name, subtype = get_sn_info(image)\n",
        "  if sn_name in to_add_c2:\n",
        "    move2.append(image)\n"
      ],
      "metadata": {
        "id": "Mrdh6bbrrOnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remaining_images['2'] = remaining_images['2'] + move2\n",
        "remaining_images['1'] = remaining_images['1'] + move1"
      ],
      "metadata": {
        "id": "QXJzRi5ZrrAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Having test_set and val_sets, materialize the complete splits\n",
        "def materialize_split(reduced_all_images, test_set, val_set):\n",
        "    split = {}\n",
        "\n",
        "    for cls in ['0', '1', '2']:\n",
        "        all_imgs = set(reduced_all_images[cls])\n",
        "        test_imgs = set(test_set.get(cls, []))\n",
        "        val_imgs  = set(val_set.get(cls, []))\n",
        "\n",
        "        # Safety checks\n",
        "        assert test_imgs.isdisjoint(val_imgs), f\"Overlap between test and val in class {cls}\"\n",
        "\n",
        "        train_imgs = all_imgs - test_imgs - val_imgs\n",
        "\n",
        "        split[cls] = {\n",
        "            'train': sorted(train_imgs),\n",
        "            'val': sorted(val_imgs),\n",
        "            'test': sorted(test_imgs)\n",
        "        }\n",
        "\n",
        "    return split\n",
        "\n",
        "\n",
        "# ---- build split 1 ----\n",
        "split1 = materialize_split(\n",
        "    reduced_all_images=reduced_all_images,\n",
        "    test_set=test_set2,\n",
        "    val_set=val_set1\n",
        ")"
      ],
      "metadata": {
        "id": "P3OAV2awvrGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- build split 2 ----\n",
        "split2 = materialize_split(\n",
        "    reduced_all_images=reduced_all_images,\n",
        "    test_set=test_set2,\n",
        "    val_set=val_set2\n",
        ")"
      ],
      "metadata": {
        "id": "esTrsnP6wSsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- build split 3 ----\n",
        "split3 = materialize_split(\n",
        "    reduced_all_images=reduced_all_images,\n",
        "    test_set=test_set2,\n",
        "    val_set=val_set3\n",
        ")"
      ],
      "metadata": {
        "id": "zKXUk3t-zzhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- build split 4 ----\n",
        "split4 = materialize_split(\n",
        "    reduced_all_images=reduced_all_images,\n",
        "    test_set=test_set2,\n",
        "    val_set=val_set4\n",
        ")"
      ],
      "metadata": {
        "id": "YEWXB9zbz2Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save test split\n",
        "# with open(f'{working_dir}/Splits/tvt_split4.pkl', 'wb') as f:\n",
        "#     pickle.dump(split4, f)\n"
      ],
      "metadata": {
        "id": "l1590hFuz68s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save test split\n",
        "# with open(f'{working_dir}/Splits/new_test_set.pkl', 'wb') as f:\n",
        "#     pickle.dump(test_set2, f)\n",
        "#"
      ],
      "metadata": {
        "id": "Olgl8DK3z7xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save test split\n",
        "# with open(f'{working_dir}/Splits/new_test_set.pkl', 'wb') as f:\n",
        "#     pickle.dump(test_set2, f)\n",
        "#"
      ],
      "metadata": {
        "id": "J500Yb6az8k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save test split\n",
        "# with open(f'{working_dir}/Splits/new_test_set.pkl', 'wb') as f:\n",
        "#     pickle.dump(test_set2, f)\n",
        "#"
      ],
      "metadata": {
        "id": "XuhyIAWTz70S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}